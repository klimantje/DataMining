{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-exploration\" data-toc-modified-id=\"Data-exploration-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data exploration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loading-the-data-and-formatting-the-variables-correctly\" data-toc-modified-id=\"Loading-the-data-and-formatting-the-variables-correctly-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Loading the data and formatting the variables correctly</a></span></li><li><span><a href=\"#Strategy\" data-toc-modified-id=\"Strategy-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Strategy</a></span></li><li><span><a href=\"#Base-Model\" data-toc-modified-id=\"Base-Model-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Base Model</a></span></li><li><span><a href=\"#New-heading\" data-toc-modified-id=\"New-heading-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>New heading</a></span></li><li><span><a href=\"#Exploring-the-variables\" data-toc-modified-id=\"Exploring-the-variables-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Exploring the variables</a></span><ul class=\"toc-item\"><li><span><a href=\"#Numeric-variables\" data-toc-modified-id=\"Numeric-variables-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Numeric variables</a></span></li><li><span><a href=\"#Categorical-variables\" data-toc-modified-id=\"Categorical-variables-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>Categorical variables</a></span></li></ul></li><li><span><a href=\"#Examing-the-missing-values-and-dropping-columns\" data-toc-modified-id=\"Examing-the-missing-values-and-dropping-columns-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Examing the missing values and dropping columns</a></span></li><li><span><a href=\"#Handling-missing-values\" data-toc-modified-id=\"Handling-missing-values-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Handling missing values</a></span></li><li><span><a href=\"#Exploring-the-correlation-between-variables\" data-toc-modified-id=\"Exploring-the-correlation-between-variables-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Exploring the correlation between variables</a></span></li><li><span><a href=\"#Exploration-of-correlation-with-target-variable\" data-toc-modified-id=\"Exploration-of-correlation-with-target-variable-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Exploration of correlation with target variable</a></span><ul class=\"toc-item\"><li><span><a href=\"#We-look-at-the-categorical-variables\" data-toc-modified-id=\"We-look-at-the-categorical-variables-1.9.1\"><span class=\"toc-item-num\">1.9.1&nbsp;&nbsp;</span>We look at the categorical variables</a></span></li></ul></li><li><span><a href=\"#Correlation-with-target-for-numerical-features\" data-toc-modified-id=\"Correlation-with-target-for-numerical-features-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>Correlation with target for numerical features</a></span></li></ul></li><li><span><a href=\"#Regression\" data-toc-modified-id=\"Regression-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Splitting-in-train-and-testset\" data-toc-modified-id=\"Splitting-in-train-and-testset-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Splitting in train and testset</a></span></li><li><span><a href=\"#Different-regression-model\" data-toc-modified-id=\"Different-regression-model-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Different regression model</a></span><ul class=\"toc-item\"><li><span><a href=\"#We-can-consider-this-problem-a-regression-problem-BUT-also-a-classificatino-problem-(-as-the-mood-is-integer-and-ordinal)\" data-toc-modified-id=\"We-can-consider-this-problem-a-regression-problem-BUT-also-a-classificatino-problem-(-as-the-mood-is-integer-and-ordinal)-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>We can consider this problem a regression problem BUT also a classificatino problem ( as the mood is integer and ordinal)</a></span></li></ul></li><li><span><a href=\"#Performance-on-the-testset\" data-toc-modified-id=\"Performance-on-the-testset-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Performance on the testset</a></span></li><li><span><a href=\"#Different-random-forest\" data-toc-modified-id=\"Different-random-forest-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Different random forest</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data and formatting the variables correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries for exploration\n",
    "import pandas as pd\n",
    "#pandas settings\n",
    "pd.set_option(\"display.max_columns\", 150)\n",
    "pd.set_option(\"display.max_info_columns\", 150)\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler, Imputer, Normalizer\n",
    "from scipy import stats\n",
    "\n",
    "# plotting missing values\n",
    "import missingno as msno\n",
    "\n",
    "#plotting inline matplotlib\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import operator\n",
    "\n",
    "#Visualisation of models\n",
    "from yellowbrick.features import FeatureImportances\n",
    "from yellowbrick.regressor import PredictionError, ResidualsPlot\n",
    "from yellowbrick.regressor.alphas import AlphaSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../ML4QS/PythonCode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.VisualizeDataset import VisualizeDataset #cant get this to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data as a pandas dataframe and exampine the different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mood = pd.read_csv(\"../Data/dataset_mood_smartphone.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a temporal dataset without many features at first sight, however to use it in a predictive model we want to transform it. We can see that for every patient at different times different variables are logged, such as call activity, arousal, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mood.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the time into actual pandas datetime format and also split date and time in different columns to help us further aggregate per time or per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formatting of date time fields\n",
    "def format_date(df):\n",
    "    #parse string as datetime\n",
    "    df[\"time\"] = df[\"time\"].apply(lambda x: \\\n",
    "    datetime.strptime(x,\"%Y-%m-%d %H:%M:%S.%f\"))  \n",
    "    #split date and time so that we can use both lateer\n",
    "    new_dates, new_times = zip(*[(d.date(), d.time()) for d in df['time']])\n",
    "    df = df.assign(new_date=new_dates, new_time=new_times)\n",
    "    df.drop(\"time\", axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mood = format_date(mood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mood.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the size of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mood.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have data for 27 unique patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mood.id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there are 19 variables measured, under which the mood which we want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mood.variable.nunique())\n",
    "print(mood.variable.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mood.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mood[(mood.variable.str.contains(\"circumplex\")) & (~mood.value.isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mood.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mood[mood.variable=='activity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the different variables were started to be logged on different days... We have to deal with that somehow or throw away some lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mood.groupby(['id', 'variable'])['new_date'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mood[(mood.variable=='mood') & (mood.id=='AS14.01')].value.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try for different lengths of period an aggregation of all variables. The aggregation type depends on the variable.\n",
    "\n",
    "- mood: average of the previous period\n",
    "- arousal: average of the previous period\n",
    "- valence: average of the previous period\n",
    "- activity: average of the previous period\n",
    "- screen: total screen activity so the sum\n",
    "- call: number of calls made so the sum\n",
    "- sms: number of sms so the sum\n",
    "- for all apps: total duration so the sum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For variables related to duration it seems logical to take the total time for the previous day (or previous days depending on the window). However, it might also be that the screen time before bed impacts the mood the next day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a method to preprocess the data, this includes:\n",
    "- pivoting the variables so that we get a feature per variable\n",
    "- aggregating per day\n",
    "- taking moving averages or sum on a defined backwards period\n",
    "- define the new target variable as the average mood over the day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We create a dictionary for aggregation and for windowing so that we can play with the windows later**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agg_columns = list(mood.variable.unique())\n",
    "agg_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start we take sum for the durations and mean for the ordinal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_dict = {}\n",
    "for x in agg_columns:\n",
    "    if x in ['mood', 'circumplex.arousal', 'circumplex.valence', 'activity']:\n",
    "        agg_dict.update({x: np.nanmean})\n",
    "    else: agg_dict.update({x:np.nansum})\n",
    "agg_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lag is 5 days for the ordinal values and 1 for the duration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_dict = {}\n",
    "for x in agg_columns:\n",
    "    if x in ['mood', 'circumplex.arousal', 'circumplex.valence', 'activity']:\n",
    "        window_dict.update({x: 5})\n",
    "    else: window_dict.update({x:1})\n",
    "window_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_time_data(dataframe, list_of_deltas, list_of_aggregations):    \n",
    "    # first pivot the variables so that we get one record per datetime and patient (get out time?)\n",
    "    pivot_df= dataframe.pivot_table(values='value', index=['id', 'new_date', 'new_time'], \\\n",
    "                                    columns='variable')\n",
    "    #DO NOT FILL NAN so that pandas only counts the nonzero ones in the means\n",
    "    #pivot_df.fillna(0, inplace=True)\n",
    "    #aggregate per day depending on the aggregation type\n",
    "    daily_df = pivot_df.groupby(['id', 'new_date']).agg(list_of_aggregations)\n",
    "    #Make mood the target variable\n",
    "    #daily_df.['target'] = daily_df['mood']\n",
    "    # shift the other variables 1 day to create new target and take only the previous days into account\n",
    "    daily_df_shift = daily_df.groupby(['id'])[list(list_of_aggregations.keys())].shift(1)\n",
    "    #add the target variable (unshifted)\n",
    "    daily_df_shift['target'] = daily_df['mood']\n",
    "    #window with the deltas and necessary aggregations\n",
    "    df_rolling = daily_df_shift\n",
    "    for x in list_of_deltas.keys():\n",
    "        df_rolling[x] = daily_df_shift[x].rolling(list_of_deltas.get(x)).agg(list_of_aggregations.get(x))   \n",
    "    return df_rolling.reset_index(level=['id', 'new_date'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepped_mood = aggregate_time_data(mood, window_dict, agg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepped_mood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the base model we want to just take the average mood of the past day, so we process the data only with the shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_dict_base = {}\n",
    "for x in agg_columns:\n",
    "    window_dict_base.update({x:0})\n",
    "window_dict_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepped_mood_base = aggregate_time_data(mood, window_dict_base, agg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df= mood.pivot_table(values='value', index=['id', 'new_date', 'new_time'], \\\n",
    "                                    columns='variable')\n",
    "#DO NOT FILL NAN so that pandas only counts the nonzero ones in the means\n",
    "#pivot_df.fillna(0, inplace=True)\n",
    "#aggregate per day depending on the aggregation type\n",
    "daily_mood = pivot_df.groupby(['id', 'new_date']).agg(agg_dict)\n",
    "# shift the other variables 1 day to create new target and take only the previous days into account\n",
    "daily_mood_shift = daily_mood.groupby(['id'])[list(agg_dict.keys())].shift(1)\n",
    "#add the target variable (unshifted)\n",
    "daily_mood_shift['target'] = daily_mood['mood']\n",
    "daily_mood_shift = daily_mood_shift.reset_index(level=['id', 'new_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_mood_shift_clean = daily_mood_shift[~np.isnan(daily_mood_shift.mood) & ~np.isnan(daily_mood_shift.target)]\n",
    "daily_mood_shift_clean['x']= daily_mood_shift_clean['mood']\n",
    "daily_mood_shift_clean['y']= daily_mood_shift_clean['target']\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter('x', # Horizontal axis\n",
    "           'y', # Vertical axis\n",
    "        data=daily_mood_shift_clean, )\n",
    "ax.plot([3,10], [3,10], color=\"r\", label =\"equality\")\n",
    "plt.xlabel(\"yesterdays average mood\")\n",
    "plt.ylabel(\"todays average mood\")\n",
    "plt.legend()\n",
    "plt.title(\"Visualisation of base model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = daily_mood_shift_clean[[ 'x', 'y']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(results.x-results.y)\n",
    "plt.title(\"Prediction errors distribution for the base model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can wonder what is a good treshold here. I would argue it doesn t matter that we are less than 1 off. We mainly want to predict the range of mood. Setting the treshold of error to 0.6 already gives us 66%. We could argue if it wouldn 't be better to predict the slope (negative or positive) of mood. One down might be worse than one up, especially from a psychologist perspective. This is not something we will cover in this analysis but a good next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong =len(results[np.abs(results.x - results.y)>0.7])\n",
    "right = len(results[np.abs(results.x - results.y)<0.7])\n",
    "total = len(results)\n",
    "print(\"We have \" + str(right/total) + \" right answers\")\n",
    "print(\"wrong: \" + str(wrong))\n",
    "print(\"right \"+ str(right))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is quite some activity data missing for certain days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop the rows with target nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepped_mood = prepped_mood[~np.isnan(prepped_mood.target)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepped_mood[np.isnan(prepped_mood.mood)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [0, 1, 2, 3, 5, 6, 7]\n",
    "i = 1\n",
    "# plot each column\n",
    "plt.figure()\n",
    "for group in groups:\n",
    "    plt.subplot(len(groups), 1, i)\n",
    "    plt.plot(values[:, group])\n",
    "    plt.title(dataset.columns[group], y=0.5, loc='right')\n",
    "    i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New heading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the variables\n",
    "\n",
    "We perform some basic data analysis on the different variables. Let's first look at the statistics of the numeric variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepped_mood.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most columns are numeric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepped_mood.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepped_mood.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make histograms and boxplots of the different numerical columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "app = dash.Dash()\n",
    "\n",
    "available_indicators = prepped_mood.columns\n",
    "\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.Div([\n",
    "\n",
    "        html.Div([\n",
    "            dcc.Dropdown(\n",
    "                id='xaxis-column',\n",
    "                options=[{'label': i, 'value': i} for i in available_indicators],\n",
    "                #value='Fertility rate, total (births per woman)'\n",
    "            ),\n",
    "            dcc.RadioItems(\n",
    "                id='xaxis-type',\n",
    "                options=[{'label': i, 'value': i} for i in ['Linear', 'Log']],\n",
    "                value='Linear',\n",
    "                labelStyle={'display': 'inline-block'}\n",
    "            ),\n",
    "            \n",
    "            dcc.Graph(id='histograms')\n",
    "        ],\n",
    "        style={'width': '48%', 'display': 'inline-block'}),\n",
    "\n",
    "        html.Div([\n",
    "            dcc.Dropdown(\n",
    "                id='yaxis-column',\n",
    "                options=[{'label': i, 'value': i} for i in available_indicators],\n",
    "                #value='Life expectancy at birth, total (years)'\n",
    "            ),\n",
    "            dcc.RadioItems(\n",
    "                id='yaxis-type',\n",
    "                options=[{'label': i, 'value': i} for i in ['Linear', 'Log']],\n",
    "                value='Linear',\n",
    "                labelStyle={'display': 'inline-block'}\n",
    "            ),\n",
    "            dcc.Graph(id='boxplots')\n",
    "            \n",
    "        ],style={'width': '48%', 'float': 'right', 'display': 'inline-block'})\n",
    "    ]),\n",
    "    \n",
    "    \n",
    "\n",
    "#    dcc.Slider(\n",
    " #       id='year--slider',\n",
    "  #      min=df['Year'].min(),\n",
    "   #     max=df['Year'].max(),\n",
    "    #    value=df['Year'].max(),\n",
    "     #   step=None,\n",
    "      #  marks={str(year): str(year) for year in df['Year'].unique()}\n",
    "   # )\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    dash.dependencies.Output('histograms', 'figure'),\n",
    "    [dash.dependencies.Input('xaxis-column', 'value'),\n",
    "     dash.dependencies.Input('xaxis-type', 'value'),\n",
    "     ])\n",
    "def update_graph(xaxis_column_name,\n",
    "                 xaxis_type):\n",
    "    #dff = df[df['Year'] == year_value]\n",
    "\n",
    "    return {\n",
    "        'data': [go.Histogram(\n",
    "            x=prepped_mood[xaxis_column_name]\n",
    "            #text=dff[dff['Indicator Name'] == yaxis_column_name]['Country Name'],\n",
    "        )],\n",
    "        'layout': go.Layout(\n",
    "            xaxis={\n",
    "                'title': xaxis_column_name,\n",
    "                'type': 'linear' if xaxis_type == 'Linear' else 'log'\n",
    "            },\n",
    "            margin={'l': 40, 'b': 40, 't': 10, 'r': 0},\n",
    "            hovermode='closest'\n",
    "        )\n",
    "    }\n",
    "\n",
    "@app.callback(\n",
    "    dash.dependencies.Output('boxplots', 'figure'),\n",
    "    [dash.dependencies.Input('yaxis-column', 'value'),\n",
    "    dash.dependencies.Input('yaxis-type', 'value')])\n",
    "\n",
    "\n",
    "def update_boxplots(yaxis_column_name,\n",
    "                 yaxis_type):\n",
    "    #dff = df[df['Year'] == year_value]\n",
    "\n",
    "     return {\n",
    "        'data': [go.Box(\n",
    "            y=prepped_mood[yaxis_column_name]\n",
    "            #text=dff[dff['Indicator Name'] == yaxis_column_name]['Country Name'],\n",
    "        )],\n",
    "        'layout': go.Layout(\n",
    "            yaxis={\n",
    "                'title': yaxis_column_name,\n",
    "                'type': 'linear' if yaxis_type == 'Linear' else 'log'\n",
    "            },\n",
    "            margin={'l': 40, 'b': 40, 't': 10, 'r': 0},\n",
    "            hovermode='closest'\n",
    "        )\n",
    "    }\n",
    "        \n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "app = dash.Dash()\n",
    "\n",
    "available_indicators = crimes.columns\n",
    "\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.Div([\n",
    "\n",
    "        html.Div([\n",
    "            dcc.Dropdown(\n",
    "                id='xaxis-column',\n",
    "                options=[{'label': i, 'value': i} for i in available_indicators],\n",
    "                #value='Fertility rate, total (births per woman)'\n",
    "            ),\n",
    "            dcc.RadioItems(\n",
    "                id='xaxis-type',\n",
    "                options=[{'label': i, 'value': i} for i in ['Linear', 'Log']],\n",
    "                value='Linear',\n",
    "                labelStyle={'display': 'inline-block'}\n",
    "            )\n",
    "        ],\n",
    "        style={'width': '48%', 'display': 'inline-block'}),\n",
    "\n",
    "        html.Div([\n",
    "            dcc.Dropdown(\n",
    "                id='yaxis-column',\n",
    "                options=[{'label': i, 'value': i} for i in available_indicators],\n",
    "                #value='Life expectancy at birth, total (years)'\n",
    "            ),\n",
    "            dcc.RadioItems(\n",
    "                id='yaxis-type',\n",
    "                options=[{'label': i, 'value': i} for i in ['Linear', 'Log']],\n",
    "                value='Linear',\n",
    "                labelStyle={'display': 'inline-block'}\n",
    "            )\n",
    "        ],style={'width': '48%', 'float': 'right', 'display': 'inline-block'})\n",
    "    ]),\n",
    "\n",
    "    dcc.Graph(id='indicator-graphic'),\n",
    "    dcc.Graph(id='distributions')\n",
    "\n",
    "#    dcc.Slider(\n",
    " #       id='year--slider',\n",
    "  #      min=df['Year'].min(),\n",
    "   #     max=df['Year'].max(),\n",
    "    #    value=df['Year'].max(),\n",
    "     #   step=None,\n",
    "      #  marks={str(year): str(year) for year in df['Year'].unique()}\n",
    "   # )\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    dash.dependencies.Output('indicator-graphic', 'figure'),\n",
    "    [dash.dependencies.Input('xaxis-column', 'value'),\n",
    "     dash.dependencies.Input('yaxis-column', 'value'),\n",
    "     dash.dependencies.Input('xaxis-type', 'value'),\n",
    "     dash.dependencies.Input('yaxis-type', 'value')])\n",
    "def update_graph(xaxis_column_name, yaxis_column_name,\n",
    "                 xaxis_type, yaxis_type):\n",
    "    #dff = df[df['Year'] == year_value]\n",
    "\n",
    "    return {\n",
    "        'data': [go.Scatter(\n",
    "            x=crimes[xaxis_column_name],\n",
    "            y=crimes[yaxis_column_name],\n",
    "            #text=dff[dff['Indicator Name'] == yaxis_column_name]['Country Name'],\n",
    "            mode='markers',\n",
    "            marker={\n",
    "                'size': 15,\n",
    "                'opacity': 0.5,\n",
    "                'line': {'width': 0.5, 'color': 'black'}\n",
    "            }\n",
    "        )],\n",
    "        'layout': go.Layout(\n",
    "            xaxis={\n",
    "                'title': xaxis_column_name,\n",
    "                'type': 'linear' if xaxis_type == 'Linear' else 'log'\n",
    "            },\n",
    "            yaxis={\n",
    "                'title': yaxis_column_name,\n",
    "                'type': 'linear' if yaxis_type == 'Linear' else 'log'\n",
    "            },\n",
    "            margin={'l': 40, 'b': 40, 't': 10, 'r': 0},\n",
    "            hovermode='closest'\n",
    "        )\n",
    "    }\n",
    "\n",
    "@app.callback(\n",
    "    dash.dependencies.Output('distributions', 'figure'),\n",
    "    [dash.dependencies.Input('xaxis-column', 'value'),\n",
    "    dash.dependencies.Input('xaxis-type', 'value')])\n",
    "\n",
    "\n",
    "def update_distributions(xaxis_column_name,\n",
    "                 xaxis_type):\n",
    "    #dff = df[df['Year'] == year_value]\n",
    "\n",
    "    return ff.create_distplot(\n",
    "            hist_data = [crimes[xaxis_column_name]],\n",
    "            group_labels = [crimes.columns],\n",
    "            rug_text=xaxis_column_name,\n",
    "            \n",
    "        )\n",
    "        \n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distributions** most variables seem to have exponential or log distribution, not normal. We will have to scale them to perform better in the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Plotting Numeric Features\n",
    "# Looping through and Plotting Numeric features\n",
    "\n",
    "for column in prepped_mood.select_dtypes(exclude=['object']).columns:   \n",
    "    \n",
    "    # Figure initiation\n",
    "    fig = plt.figure(figsize=(18,12))\n",
    "    ### Distribution plot\n",
    "    sns.distplot(prepped_mood[column].dropna(), ax=plt.subplot(221));\n",
    "    # X-axis Label\n",
    "    plt.xlabel(column, fontsize=14);\n",
    "    # Y-axis Label\n",
    "    plt.ylabel('Density', fontsize=14);\n",
    "    ### Boxplot\n",
    "    sns.boxplot(prepped_mood[column].dropna(), ax=plt.subplot(222));\n",
    "    # X-axis Label\n",
    "    plt.xlabel(column, fontsize=14);\n",
    "    # Y-axis Label\n",
    "    plt.ylabel(\"Box\", fontsize=14)\n",
    "    # Adding Super Title (One for a whole figure)\n",
    "    plt.suptitle('Plots for ' + column, fontsize=18);\n",
    " \n",
    "# Printing Chart\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We make a simple countplot for each variable to see how the records are distributed over the different categories. We see for instance that most places are cities. There are way too many community names to be useful. We see that the data is not sampled evenly over states and over community types (way more data point from cities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Plotting categorical Features\n",
    "# Looping through and Plotting categorical features\n",
    "\n",
    "\n",
    "for column in nominal_features:   \n",
    "    \n",
    "    # Figure initiation\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    ### Distribution plot\n",
    "    sns.countplot(crimes[column].dropna());\n",
    "    # X-axis Label\n",
    "    plt.xlabel(column, fontsize=14);\n",
    "   # Adding Super Title (One for a whole figure)\n",
    "    plt.suptitle('Count class size for ' + column, fontsize=18);\n",
    " \n",
    "# Printing Chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Examing the missing values and dropping columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We use the missing_no package to visualise the missing values. We see that some columns have too much missing values to be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "msno.matrix(crimes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#percentage nulls per column\n",
    "missing_perc = (crimes.isnull().sum()/crimes.shape[0]).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For the columns that have more than half of the values missing we create a new feature indicating whether it is missing or not and drop the original column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "crimes_helper = crimes[missing_perc[missing_perc.values>0.5].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "crimes_helper.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for col in crimes_helper.columns:\n",
    "    crimes[col+\"_missing\"] = crimes[col].apply(lambda x: np.isnan(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "crimes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "crimes.dropna(thresh = 0.5*len(crimes), axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "crimes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Handling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We still have only 1 missing value left in other per cap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "msno.matrix(crimes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#percentage nulls per column\n",
    "(crimes.isnull().sum()/crimes.shape[0]).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "only other per cap is missing in one row. But looking at the pct black and such there are no other people than black, whitem asian and hispanic (the sum is alreayd more than hundred)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "crimes.filter(regex=\"(?i)white|black|asian|other|indian|hisp\")[np.isnan(crimes.otherPerCap)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's plot the wages per capita for other. We impute by the median because of the skewedness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(crimes.dropna().otherPerCap)\n",
    "plt.axvline(crimes.dropna().otherPerCap.median(), ls='--', color ='r')\n",
    "plt.axvline(crimes.dropna().otherPerCap.mean(), ls='--', color ='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "crimes[\"otherPerCap\"] = crimes[\"otherPerCap\"].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "crimes[crimes.otherPerCap.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "No missing values are left!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "crimes.isnull().sum().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the correlation between variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now explore the correlation between the variables to see which ones we should consider for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING Correllation matrix\n",
    "corr_mat=prepped_mood.corr(method='pearson')\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(corr_mat,vmax=1,square=True,annot=False,cmap='cubehelix')\n",
    "plt.title(\"Correlation between non-class variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to drop features which are highly correlated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = crimes.drop(columns=['violentPerPop'], axis = 1).corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "#select treshhold\n",
    "\n",
    "tresh = 0.80\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.80\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > tresh)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes.drop(crimes[to_drop], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING Correllation matrix\n",
    "corr_mat=crimes.corr(method='pearson').abs()\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(corr_mat,vmax=1,square=True,annot=False)\n",
    "plt.title(\"Correlation between variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of correlation with target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, our target variable seems far from normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes.violentPerPop.plot(kind='HIST', bins = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log transform looks better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(crimes['violentPerPop'].apply(lambda x: np.log(x+1))) #plus one to avoid log zero\n",
    "plt.title(\"log transformed number of violent crimes\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make new target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes['target'] = crimes['violentPerPop'].apply(lambda x: np.log(x+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We look at the categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State and communityName have way too much "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Target variable exploration\n",
    "\n",
    "facet = sns.FacetGrid(crimes , aspect=4, hue='communityType')\n",
    "facet.map(sns.kdeplot,'target',shade= True)\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Violent crimes per community type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot(x='communityType',y='target', hue='communityType', kind=\"box\", data=crimes, size = 7, aspect = 2,)\n",
    "#limit y axis for visibility\n",
    "\n",
    "\n",
    "plt.title(\"violent crimes per community type\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that cities have higher violent crime rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation with target for numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the mood for the past 5 days and valence have the highest absolute correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepped_mood.corr().abs()['target'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting in train and testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use one hot encoding to convert the nominal variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepped_mood.drop(columns=['new_date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_features = prepped_mood.select_dtypes(include='O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don t need the date anymore now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepped_mood = pd.get_dummies(prepped_mood, columns=nominal_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepped_mood.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can consider this problem a regression problem BUT also a classificatino problem ( as the mood is integer and ordinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to center and scale age and fare. We hence setup an scikit-learn pipeline. We make a list of models to try:\n",
    "- simple regression\n",
    "- penalized regression\n",
    "- Random Forest\n",
    "- Neural network\n",
    "- KNN\n",
    "- XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset in a test and trainset. Our testset size is 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing vars\n",
    "X_train, X_test, y_train, y_test = train_test_split(prepped_mood.drop(axis=1, columns=['target']), prepped_mood['target'], test_size=0.5)\n",
    "\n",
    "\n",
    "models=dict()\n",
    "pipelines = dict()\n",
    "#Logistic Regression\n",
    "models.update({\"LinearRegression\": LinearRegression()})\n",
    "models.update({\"Elastic Net\": ElasticNet()})   \n",
    "models.update({\"XGBoost\": XGBRegressor()})\n",
    "models.update({\"Random Forest\": RandomForestRegressor(n_estimators=1000,random_state=0,n_jobs=-1)})\n",
    "models.update({\"Gradient Boost\": GradientBoostingRegressor()})\n",
    "for i in range(2,40):\n",
    "    models.update({\"KNN\" + str(i) : KNeighborsRegressor(n_neighbors=i)})\n",
    "models.update({\"Neural Net\": MLPRegressor()})\n",
    "\n",
    "# Compute train and test errors\n",
    "train_errors = dict()\n",
    "test_errors = dict()\n",
    "imputer = Imputer(missing_values=\"NaN\", axis=0,  strategy=\"mean\", verbose=5)\n",
    "preprocessors = [(\"imputer\", imputer), ('reduce_dim', PCA()), ('scaler',StandardScaler()),('normalizer', Normalizer())]\n",
    "\n",
    "for label, clf in models.items():\n",
    "    estimators = preprocessors +  [('clf',clf )]\n",
    "    pipeline = Pipeline(estimators) #standard scale\n",
    "    pipelines.update({label:pipeline})\n",
    "    scores = cross_val_score(\n",
    "        estimator=pipeline,\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        cv=10,\n",
    "        scoring='neg_mean_squared_error',\n",
    "    )\n",
    "    pipeline.fit(X_test, y_test)\n",
    "    test_errors.update({label: mean_squared_error(y_test, pipeline.predict(X_test))})\n",
    "    pipeline.fit(X_train,y_train)\n",
    "    r2 = r2_score(y_train, pipeline.predict(X_train))\n",
    "    print(pipeline.feature_importances_)\n",
    "    print(\"Neg MSE: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "    print(\"r2: %0.2f [%s]\" % (r2, label))\n",
    "    print(\"test error: %0.2f\" % test_errors.get(label))\n",
    "    \n",
    "best_model = min(test_errors.items(), key=operator.itemgetter(1))[0]\n",
    "print(\"Best model with lowest test error : %s\" % best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Plot feature importance\n",
    "clf= pipelines.get(\"Random Forest\").named_steps[\"clf\"]\n",
    "from yellowbrick.features import FeatureImportances\n",
    "# Create a new matplotlib figure\n",
    "fig = plt.figure(figsize=(10,20))\n",
    "viz = FeatureImportances(clf,)\n",
    "viz.fit(X_train, y_train)\n",
    "viz.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=models.get(\"KNN1\")\n",
    "\n",
    "estimators = [ ('scaler',StandardScaler()),('normalizer', Normalizer()), ('clf',clf )]\n",
    "pipeline = Pipeline(estimators) #standard scale\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_train)\n",
    "pd.DataFrame(data={'predictions': y_pred, 'actual': y_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R squared: -167521.55 (+/- 42890.81) [LinearRegression]\n",
    "test error: 205645.63\n",
    "R squared: -232087.68 (+/- 62426.01) [Elastic Net]\n",
    "test error: 295655.91\n",
    "R squared: -148197.50 (+/- 37426.63) [XGBoost]\n",
    "test error: 76787.41\n",
    "R squared: -138506.75 (+/- 34178.45) [Random Forest]\n",
    "test error: 25326.34\n",
    "R squared: -149016.16 (+/- 34920.26) [Gradient Boost]\n",
    "test error: 74549.01\n",
    "R squared: -151117.59 (+/- 35133.19) [KNN]\n",
    "test error: 164439.03\n",
    "R squared: -348716.62 (+/- 87084.29) [Neural Net]\n",
    "test error: 436115.04\n",
    "Best model with lowest test error : Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_errors)\n",
    "print(test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Instantiate the linear model and visualizer\n",
    "for label, clf in models.items():\n",
    "    estimators = [('reduce_dim', PCA()), ('scaler',StandardScaler()),('normalizer', Normalizer()), ('clf',clf )]\n",
    "    pipeline = Pipeline(estimators) #standard scale    \n",
    "    visualizer1 = PredictionError(pipeline)\n",
    "    visualizer1.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "    visualizer1.score(X_test, y_test)\n",
    "    visualizer = ResidualsPlot(pipeline)\n",
    "    visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "    visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "    g1 = visualizer.poof() # Evaluate the model on the test data\n",
    "    g = visualizer.poof()             # Draw/show/poof the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.features import FeatureImportances\n",
    "# Create a new matplotlib figure\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "for label, clf in models.items():\n",
    "    pipeline = Pipeline([('scaler',StandardScaler()),('normalizer', Normalizer()), ('clf',clf )]) #standard scale\n",
    "    viz = FeatureImportances(pipeline, ax=ax)\n",
    "    viz.fit(X_train, y_train)\n",
    "    viz.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on the testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check now the performance on the testset and see that KNN does not perform very well. Random forest seems the most promising so we will tweak that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Compute train and test errors\n",
    "train_errors = list()\n",
    "test_errors = list()\n",
    "for label, clf in models.items():\n",
    "    .fit(X_train, y_train)\n",
    "    train_errors.append(enet.score(X_train, y_train))\n",
    "    test_errors.append(enet.score(X_test, y_test))\n",
    "\n",
    "i_alpha_optim = np.argmax(test_errors)\n",
    "alpha_optim = alphas[i_alpha_optim]\n",
    "print(\"Optimal regularization parameter : %s\" % alpha_optim)\n",
    "\n",
    "# Estimate the coef_ on full data with optimal regularization parameter\n",
    "enet.set_params(alpha=alpha_optim)\n",
    "coef_ = enet.fit(X, y).coef_\n",
    "\n",
    "# #############################################################################\n",
    "# Plot results functions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.semilogx(alphas, train_errors, label='Train')\n",
    "plt.semilogx(alphas, test_errors, label='Test')\n",
    "plt.vlines(alpha_optim, plt.ylim()[0], np.max(test_errors), color='k',\n",
    "           linewidth=3, label='Optimum on test')\n",
    "plt.legend(loc='lower left')\n",
    "plt.ylim([0, 1.2])\n",
    "plt.xlabel('Regularization parameter')\n",
    "plt.ylabel('Performance')\n",
    "\n",
    "# Show estimated coef_ vs true coef\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(coef, label='True coef')\n",
    "plt.plot(coef_, label='Estimated coef')\n",
    "plt.legend()\n",
    "plt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.26)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rfc = RandomForestRegressor(n_jobs=-1,  oob_score = True) \n",
    "pipeline = Pipeline([('scaler',StandardScaler()),('normalizer', Normalizer()), ('clf',rfc )]) #standard scale\n",
    "# Use a grid over parameters of interest\n",
    "param_grid = {\"clf__n_estimators\":[10,100,1000],\n",
    "    \"clf__max_depth\": [3, None],\n",
    "              \"clf__max_features\": [\"sqrt\", None, \"log2\"],\n",
    "              \"clf__min_samples_split\": [2, 3, 10],\n",
    "              \"clf__min_samples_leaf\": [1, 3, 10],\n",
    "              # \"bootstrap\": [True, False],\n",
    "              #\"clf__criterion\": [\"gini\", \"entropy\"]\n",
    "             }\n",
    "#pipeline.get_params().keys()\n",
    "CV_rfc = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv= 10, scoring = 'mean_squared_error')\n",
    "CV_rfc.fit(X_train, y_train)\n",
    "print(CV_rfc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Plot feature importance\n",
    "clf= CV_rfc.best_estimator_.steps[2][1]\n",
    "from yellowbrick.features import FeatureImportances\n",
    "# Create a new matplotlib figure\n",
    "fig = plt.figure(figsize=(10,20))\n",
    "\n",
    "viz = FeatureImportances(clf,)\n",
    "viz.fit(X_train, y_train)\n",
    "viz.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of alphas to cross-validate against\n",
    "alphas = np.logspace(-12, -0.5, 400)\n",
    "\n",
    "# Instantiate the linear model and visualizer\n",
    "model = LassoCV(alphas=alphas)\n",
    "visualizer = AlphaSelection(model)\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "g = visualizer.poof()             # Draw/show/poof the data"
   ]
  }
 ],
 "metadata": {
  "author": "An De Rijdt",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "244px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
